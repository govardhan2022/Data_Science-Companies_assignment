{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c7f780e-0000-486b-99f5-dde153733823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from newspaper import Article\n",
    "import lxml.html.clean\n",
    "\n",
    "import requests\n",
    "\n",
    "import re\n",
    "import os  # Import os to handle file operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa703764-f446-4391-8398-8bb8ae00b93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy\n",
      "\n",
      "Text:\n",
      " Client Background\n",
      "\n",
      "Client: A leading IT & tech firm in the USA\n",
      "\n",
      "Industry Type: IT\n",
      "\n",
      "Products & Services: IT Consulting, IT Support, SaaS, Marketing Strategy\n",
      "\n",
      "Organization Size: 10+\n",
      "\n",
      "The Problem\n",
      "\n",
      "Building AI and ML based YouTube analytics and content creation tool that will help youtuber to understand their subscriberâ€™s watching behaviour, help them in content research, creation and publication.\n",
      "\n",
      "Our Solution\n",
      "\n",
      "Created a MERN stack web application and integrated AI models to helps youtuber to generated titles, descriptions, tags, hashtags, captions etc. Help them to check thumbnail quality, analysis on the videos using video auditor tool, analysis on comments using sentiments analysis, help to under their subscribers using churn predication AI model.\n",
      "\n",
      "Solution Architecture\n",
      "\n",
      "https://www.figma.com/file/WQs01mmmNBZ1SjNE2IV8Sl/Youtube-Web-App-By-SHiV?type=design&node-id=0-1&mode=design&t=Lh2jRx4bGQq6l4WU-0\n",
      "\n",
      "Deliverables\n",
      "\n",
      "Web Applications\n",
      "\n",
      "Supports\n",
      "\n",
      "Maintenance\n",
      "\n",
      "Feature Enhancement\n",
      "\n",
      "Tech Stack\n",
      "\n",
      "Tools used\n",
      "\n",
      "VS code\n",
      "\n",
      "Language/techniques used\n",
      "\n",
      "React.js\n",
      "\n",
      "Express.js\n",
      "\n",
      "Node.js\n",
      "\n",
      "Python\n",
      "\n",
      "Models used\n",
      "\n",
      "Python libraries\n",
      "\n",
      "Skills used\n",
      "\n",
      "Data scientise\n",
      "\n",
      "Full Stack developer\n",
      "\n",
      "Databases used\n",
      "\n",
      "MongoDB\n",
      "\n",
      "Web Cloud Servers used\n",
      "\n",
      "Google Cloud Platform\n",
      "\n",
      "Project Snapshots\n",
      "\n",
      "Home Page\n",
      "\n",
      "Tool Page\n",
      "\n",
      "Dashboard\n",
      "\n",
      "Blog Page\n",
      "\n",
      "Single Blog Post\n",
      "\n",
      "About Us\n",
      "\n",
      "Contact Us\n",
      "\n",
      "Login Page\n",
      "\n",
      "Title and Description tool Page\n",
      "\n",
      "Thumbnail Quality check tool\n",
      "\n",
      "Project website url\n",
      "\n",
      "https://tubetool.ai\n",
      "\n",
      "Summarize\n",
      "\n",
      "Summarized: https://blackcoffer.com/\n",
      "\n",
      "This project was done by the Blackcoffer Team, a Global IT Consulting firm.\n",
      "\n",
      "Contact Details\n",
      "\n",
      "This solution was designed and developed by Blackcoffer Team\n",
      "\n",
      "Here are my contact details:\n",
      "\n",
      "Firm Name: Blackcoffer Pvt. Ltd.\n",
      "\n",
      "Firm Website: www.blackcoffer.com\n",
      "\n",
      "Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043\n",
      "\n",
      "Email: ajay@blackcoffer.com\n",
      "\n",
      "Skype: asbidyarthy\n",
      "\n",
      "WhatsApp: +91 9717367468\n",
      "\n",
      "Telegram: @asbidyarthy\n",
      "Extracted content saved to Netclan20241017.txt\n"
     ]
    }
   ],
   "source": [
    "# Specify the URL of the article\n",
    "url = \"https://insights.blackcoffer.com/ai-and-ml-based-youtube-analytics-and-content-creation-tool-for-optimizing-subscriber-engagement-and-content-strategy/\"\n",
    "\n",
    "# Create an Article object\n",
    "article = Article(url)\n",
    "\n",
    "# Download and parse the article\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "# Perform NLP on the article (optional)\n",
    "article.nlp()\n",
    "\n",
    "# Extract title and text\n",
    "title = article.title\n",
    "text = article.text\n",
    "\n",
    "# Print title and text (optional)\n",
    "print(\"Title:\", title)\n",
    "print(\"\\nText:\\n\", text)\n",
    "\n",
    "# Save extracted content to a text file with URL_ID as filename\n",
    "url_id = \"Netclan20241017\"  # You can modify this to use a specific URL_ID if needed\n",
    "with open(f\"{url_id}.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"Title: {title}\\n\\n{text}\")\n",
    "\n",
    "print(f\"Extracted content saved to {url_id}.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cffa649-fb6d-4fca-9f50-70498430edfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d579e99b-3ea8-4917-a98c-78d773125b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop words\n",
    "stop_words = set()\n",
    "for file in [\"Complete_StopWords\"]:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        stop_words.update(word.strip().lower() for word in f)\n",
    "\n",
    "# Load positive and negative words\n",
    "positive_words = set()\n",
    "negative_words = set()\n",
    "with open(\"positive-words\", \"r\") as f:\n",
    "    positive_words.update(word.strip().lower() for word in f if word.strip() not in stop_words)\n",
    "with open(\"negative-words\", \"r\") as f:\n",
    "    negative_words.update(word.strip().lower() for word in f if word.strip() not in stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "412326a3-b0f3-4b26-be53-0cf90d257635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    # Tokenize words and sentences\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Remove stop words\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "    return cleaned_words, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6936e65b-28f5-47c4-ae6a-d0ea3697fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment_scores(words):\n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = sum(1 for word in words if word in negative_words)\n",
    "    return positive_score, negative_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd3849e0-21ef-412b-96f9-b69348f2688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_polarity_subjectivity(positive_score, negative_score, total_words):\n",
    "    polarity = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "    subjectivity = (positive_score + negative_score) / (total_words + 0.000001)\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51e16539-0367-4178-8306-322ea6886375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_readability_metrics(words, sentences):\n",
    "    total_words = len(words)\n",
    "    total_sentences = len(sentences)\n",
    "    avg_sentence_length = total_words / total_sentences if total_sentences > 0 else 0\n",
    "    avg_words_per_sentence = total_words / total_sentences if total_sentences > 0 else 0\n",
    "    complex_words = sum(1 for word in words if len(re.findall(r'[aeiouy]', word)) > 2)\n",
    "    percentage_complex_words = complex_words / total_words if total_words > 0 else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    return avg_sentence_length, avg_words_per_sentence, percentage_complex_words, fog_index, complex_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f264e0b1-27a9-43be-8159-0d615d7ad940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_stats(words):\n",
    "    total_words = len(words)\n",
    "    avg_word_length = sum(len(word) for word in words) / total_words if total_words > 0 else 0\n",
    "    return total_words, avg_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28fb4c1d-3d66-45ab-9a4f-d1d5aa884769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    syllables = len(re.findall(r'[aeiouy]', word))\n",
    "    if word.endswith(('es', 'ed')):\n",
    "        syllables = max(1, syllables - 1)\n",
    "    return syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75623350-453b-4a8f-b765-ce96deb8b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_personal_pronouns(text):\n",
    "    pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.IGNORECASE)\n",
    "    return len(pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "134bbf8b-7173-4926-86b6-aac14945e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    cleaned_words, sentences = clean_text(text)\n",
    "    positive_score, negative_score = calculate_sentiment_scores(cleaned_words)\n",
    "    polarity, subjectivity = calculate_polarity_subjectivity(positive_score, negative_score, len(cleaned_words))\n",
    "    avg_sentence_length, avg_words_per_sentence, percentage_complex_words, fog_index, complex_word_count = calculate_readability_metrics(cleaned_words, sentences)\n",
    "    total_words, avg_word_length = calculate_word_stats(cleaned_words)\n",
    "    syllable_per_word = sum(syllable_count(word) for word in cleaned_words) / total_words if total_words > 0 else 0\n",
    "    personal_pronouns = count_personal_pronouns(text)\n",
    "    \n",
    "    return {\n",
    "        \"URL_ID\": file.split(\".\")[0],\n",
    "        \"url\": url,\n",
    "        \"Positive Score\": positive_score,\n",
    "        \"Negative Score\": negative_score,\n",
    "        \"Polarity Score\": polarity,\n",
    "        \"Subjectivity Score\": subjectivity,\n",
    "        \"Average Sentence Length\": avg_sentence_length,\n",
    "        \"Average Words Per Sentence\": avg_words_per_sentence,\n",
    "        \"Percentage of Complex Words\": percentage_complex_words,\n",
    "        \"Fog Index\": fog_index,\n",
    "        \"Complex Word Count\": complex_word_count,\n",
    "        \"Word Count\": total_words,\n",
    "        \"Syllable Per Word\": syllable_per_word,\n",
    "        \"Personal Pronouns\": personal_pronouns,\n",
    "        \"Average Word Length\": avg_word_length,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76a0fff7-46c3-4323-998a-18ef3ce61d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        results.append(analyze_text(text))\n",
    "        \n",
    "# Save results to Excel\n",
    "output_df = pd.DataFrame(results)\n",
    "output_df.to_csv(\"Output Data Structure5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0803166-21de-4f84-9dc1-f8b3d8fc0ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
